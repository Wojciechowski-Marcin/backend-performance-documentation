% !TeX root = ./0Base.tex

\chapter{Criteria description}

\section{Performance benchmark}

\subsection{Scenarios}
The task for each application is to complete simple CRUD operations as fast as possible. For comparison of the performance of the applications, a few scenarios were developed:

\begin{itemize}
    \item retrieving multiple objects (getMany)
    \item retrieving single object (get)
    \item updating a single object (put)
    \item creating a single object (post)
    \item deleting a single object (delete)
\end{itemize}

Scenarios were tested with a few different application loads, which are represented by a number of virtual users (VUs) - as mentioned in the k6 repository description they are glorified, parallel while(true) loops. 
% TODO annotation to k6 repo


\subsection{Database snapshots}

To avoid any differences in the database between the tests, at the beginning of the script the database is populated and the snapshot is stored locally. Before each test, the snapshot is restored.

% TODO add listing from script to the create and restore snapshot functions

\subsection{Application isolation}

To be sure that the applications are running in an isolated environment, docker containers were used. To simplify the research, a Docker Compose configuration was prepared, that builds and starts all the necessary containers at once.

% TODO reference docker-compose script, write about the dockerfiles

\subsection{Test progress}

The measures are gathered from each application in the following manner:

\input{pseudocodes/testProgress}

\subsection{Software versions and hardware}

The tests were run on a laptop with the following specification:

\input{tables/hardware}

Frameworks used to build the application were in the following versions:

\input{tables/software}

\section{Security}
