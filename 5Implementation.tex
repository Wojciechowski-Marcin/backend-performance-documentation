% !TeX root = ./0Base.tex

\chapter{Implementation}

%
% DOCKER
%
\section{Docker and Docker Compose}
\subsection{\acrlong{pg}}
\acrlong{pg} is built straight from the image without the need of Dockerfile, thus only the Docker Compose configuration is present. It places environment variables described in section \ref{sec:pgDbVars} from the file, creates a named volume containing the database files, reserves memory and creates a healthcheck, which allows to determine whether the container is ready to be connected to. Configuration for this service is presented in listing \ref{lst:postgresDockerCompose}
\input{listings/postgresDockerCompose.tex}

\subsection{Applications}
It made utmost sense to place this section standalone and not within each application, since the configurations for all of them are very alike.
Dockerfiles are configured to install all necessary packages, place the code in suitable folders, specify a non-root user for the container to run as and set the entrypoint to a script that starts the application.
The main differences between the Dockerfiles are the chosen images:
\begin{itemize}
    \item Django Dockerfile was inspired by Anuj Sharma, who created one for his series of Django development guide articles \cite{djangoDockerfile} - image used is python:3.9.1-slim,
    \item For Express.js, a latest slim image of \acrshort{lts} node version was chosen - node:14.17.0-slim,
    \item ASP.NET Core has a Dockerfile suggested by the documentation, so it was used in the application - images dotnet:2.1-sdk and dotnet:2.1-aspnetcore-runtime (multistage build) \cite{aspnetDockerfile}.
\end{itemize}
Dockerfiles were also checked by Hadolint, which is a linter that helps to build best practice Docker images \cite{hadolintGit}.
It verifies if the Dockerfile follows the rules presented in official docker documentation \cite{dockerBestPractices}.
Example Dockerfile is presented in listing \ref{lst:expressDockerfile}.
Docker Compose configuration points to the build folder, imports the environment variables for connection with \acrshort{pg} and variable with amount of users to create, ensures that the application starts after the database has started (makes the app wait for \acrlong{pg} healthcheck) and reserves memory for the application. To see an example of Docker Compose configuration check listing \ref{lst:expressDockerCompose}.
\input{listings/expressDockerfile.tex}
\input{listings/expressDockerCompose.tex}

\subsection{K6 and related tools}
Developers of k6 prepared instructions for usage with Docker \cite{k6RunningLocalTests}.
as well as an example Docker Compose configuration, including InfluxDB and Grafana integration \cite{k6DockerCompose}.
In the interest of the performance tests the latter was introduced, with small adjustments to fit the needs of test environment. For example, Grafana configuration was not needed in our case, since the results are later exported to a file for drawing custom graphs.

%
% DJANGO
%
\section{Django}
\subsection{Model}\label{sub:djangoModel}
Django offers a built in User model, however it was decided not to use it in this case. The reason for that is that the built in model handles extra operations for the User, like creating groups, permissions, authentication and a few additional fields. Instead of using it, the implementation of model presented in listing \ref{lst:djangoModel} was created. It does not contain the primary key definition, as django.db.models.Model class handles it automatically.
\input{listings/djangoModel.tex}

\subsection{Database connection and initialization}
Django handles a lot of things for the user - connection is very simple - in initial project generation a file settings.py is created, which consists of all the necessary variables for the system to work. In the file we can find a variable named Databases, initially with SQLite backend. All that needs to be done to connect to our \acrlong{pg} is to change the engine to built-in \acrshort{pg} backend and change the remaining fields - name, user, password, host and port, as presented in listing \ref{lst:djangoDbConn}. With that done, the connection is automatically done on the system startup.
\input{listings/djangoDbConnection.tex}
Creating the table is handled by migration system - to create the migrations the command from listing \ref{lst:djangoMakeMigrations} had to be run.
\input{listings/djangoMakeMigrations.tex}
This creates the tables based on the model presented in models.py files through the whole project. In this case, it only created one table.
For creating the initial population a management function was prepared, that is being executed from the main script. Seeding the database was presented in listing \ref{lst:djangoSeedDb}.
\input{listings/djangoSeedDb.tex}

\subsection{Routing and serialization}
Django routing is to be placed in urls.py files. There is one main file in the project configuration folder, and one in each module. Since this application consists of only one modules, two urls.py files exist - presented in listings \ref{lst:djangoUrlsConfig} and \ref{lst:djangoUrlsApp}.
\input{listings/djangoUrlsConfig.tex}
\input{listings/djangoUrlsApp.tex}
Serialization is another great thing about Django and \acrlong{drf} - \acrshort{drf} has built in abstract ModelSerializer class - to create a serializer for our model, all that needs to be done is to specify which model and which fields we want to serialize, as presented in listing \ref{lst:djangoSerialization}.
\input{listings/djangoSerialization.tex}

\subsection{Endpoints}
It is pretty certain at this point that django offers a lot of functionality. It should come with no surprise that the endpoints can also be implemented with a few lines of code, thanks to the built in methods. As shown in the listing \ref{lst:djangoViews}, creating views does not require much, only the queryset containing all user models and a serializer class.  With this ViewSet and one standalone function we get all the endpoints described in section \ref{sec:endpoints} of this document. UserViewSet class was used in the routing in file presented in listing \ref{lst:djangoUrlsApp}.
\input{listings/djangoViews.tex}

%
% EXPRESS
%
\section{Express}
\subsection{Model}
User model in express.js needs to contain all the necessary logic for handling \acrshort{crud} operations. Implementation of the model is shown in listing \ref{lst:expressModel} - it shows all \acrshort{sql} statements except include, which is placed in separate file. Database queries return \acrshort{json} object that is ready to be
\input{listings/expressModel.tex}

\subsection{Database connection and initialization}
Work with express is a bit more difficult, as most of the configuration needs to be done by the user. To connect with the database, a postgres promise instance needs to be created. Because there is no migration system, creating the table also needs to be done manually. To do so, a database configuration file was created (listing \ref{lst:expressDbConnection}) and imported into the entrypoint file. It contains all the necessary information about the connection and creation of the table.
\input{listings/expressDbConnection.tex}
Initialization of the database is handled by function presented in listing \ref{lst:expressSeedDb}.
\input{listings/expressSeedDb.tex}

\subsection{Routing and endpoints}
Unlike in Django, routes and endpoints are not separated in two files. It is very common to keep them in a single file, as shown in listing \ref{lst:expressEndpoints}. All they do is registering a route on the provided path in the first argument and executing the functions provided in the second argument. The mentioned functions call the respective method from User model sends appropriate response based on the return data of the query.
\input{listings/expressEndpoints.tex}

%
% ASP.NET
%
\section{ASP.NET}
\subsection{Model}
Model in ASP.NET Core is a class that serves a similar purpose to Django model (shown in subsection \ref{sub:djangoModel}). It defines all the fields that need to be placed in the database model, including the names, types and constraints, using 'Required' and 'Column' decorators. In addition to that, using 'JsonProperty' decorator we can describe a field name that is later used in serialization and deserialization, so we do not have to specify a serializer manually later. Model is presented in listing \ref{lst:aspnetModel}.
\input{listings/aspnetModel.tex}

\subsection{Database connection and initialization}
Connection to the database looked more similar in Express.js - we need to get all environment variables, create a connection string string using them and pass them to the \acrlong{pg} provider, as shown in listing \ref{lst:aspnetDbConnection}. Initial population generation is shown in \ref{lst:aspnetSeedDb}.
\input{listings/aspnetDbConnection.tex}
\input{listings/aspnetSeedDb.tex}

\subsection{Routing and Endpoints}
The code for controllers is quite lengthy, but that is because routing and endpoint definition is a part of database context class - using 'Route', 'HttpGet', 'HttpPost', 'HttpPut' and 'HttpDelete' decorators we are able to define routes for given functions. Request parameters are placed directly into the function arguments, using 'FromQuery', 'FromBody' clauses or by putting the argument in the decorator (for example 'HttpGet("{id}")'). All controllers can be seen in listing \ref{lst:aspnetEndpoints}.
\input{listings/aspnetEndpoints.tex}


%
% PERFORMANCE
%
\section{Performance tests}
Tests implementation is divided into 6 files:
\begin{itemize}
    \item config.js,
    \item delete.test.js,
    \item get.test.js,
    \item getMany.test.js,
    \item post.test.js,
    \item put.test.js.
\end{itemize}
Configuration file config.js is a module that contains common code for all tests, utility functions such as waiting for container, preparing scenarios, parsing environment variables, creating users (in the same fashion as ones existing in the database) or exporting results to a file.

\subsection{Environment variables}
From the main script, the following test variables are passed:
\begin{itemize}
    \item TEST\_TIME - defines the length of a performance test,
    \item VU\_AMOUNT - shows with how many concurrent \acrshort{vu}s the test will be run,
    \item USER\_AMOUNT - describes amount of users existing in the database,
    \item SCENARIO - is the current scenario (according to what was said in subsection \ref{sub:scenarios}, for 1 \acrshort{vu} the scenario is constant-vus and for the other cases it is ramping-vus),
    \item FRAMEWORK - shows which framework is currently started,
    \item RESULTS\_PATH - defines a path to a file for exporting final results.
\end{itemize}

\subsection{Scenarios}
K6 have a few executors defined. For this research only two of them will be used:
\begin{itemize}
    \item Constant \acrshort{vu}s, where a fixed number of \acrlong{vu}s try to execute as many iterations as possible within a specified period of time,
    \item and Ramping \acrshort{vu}s, which is very alike to the Constant \acrshort{vu}s executor, but works on a variable number of \acrlong{vu}s \cite{k6Executors}.
\end{itemize}
To further demonstrate the meaning of code in listing \ref{lst:k6Scenarios}, it needs to be mentioned that graceful stop and graceful ramp down variables allow to finish currently running requests after the time limit has passed and stages in 'ramping-vus' case statement describe the \acrshort{vu} chart from figure \ref{fig:vusPerSecond}.
\input{listings/k6Scenarios.tex}

\subsection{Setup}
Setup function is the first user-defined thing that runs when the k6 application is started. It needs to be run from the test file, but the code that is common for all tests is shown in \ref{lst:k6Setup}. This function sends requests every 3 seconds, trying to get a response from applications' status endpoint.
\input{listings/k6Setup.tex}

\subsection{Get test}
Get test is the simplest of the group, so it is going to be used to explain the whole k6 test structure.
\input{listings/k6Get.tex}
%
% MAIN SCRIPT
%
\section{Main script}
